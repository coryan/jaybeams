## Report for `r basename(data.filename)`


```{r include=FALSE}
# First we need to load the libraries that we need for this analysis:
library(boot)
library(ggplot2)
library(parallel)
library(pwr)
library(knitr)
# We also create some constants that make the graphs look pretty:
golden.ratio <- (1 + sqrt(5.0)) / 2
# ... the desired width and height, in inches ...
svg.w <- 8.0
svg.h <- svg.w / golden.ratio
# ... same thing, but in pixels ...
png.w <- 960
png.h <- round(png.w / golden.ratio)
# We do not want the output to be too wide, because it looks wrong in
# the rendered web pages:
options(width=60, digits=4)

# We want to stop on the first error, because it is easy to miss them otherwise
knitr::opts_chunk$set(error=FALSE, fig.height=svg.h, fig.width=svg.w)
```

We load the file, which has a well-known name, into a `data.frame`
structure:

```{r}
data <- read.csv(
    file=data.filename, header=FALSE, comment.char='#',
    col.names=c('copy', 'lib', 'precision', 'reduction', 'size', 'nanoseconds'))
```

The raw data is in nanoseconds, I prefer microseconds because they are
easier to think about:

```{r}
data$microseconds <- data$nanoseconds / 1000.0
```

At this point I am curious as to how the data looks like, and probably
you too, first just the usual summary:

```{r summary.table}
data.summary <- aggregate(
    microseconds ~ reduction + copy, data=data, FUN=summary)
kable(cbind(
    as.character(data.summary$reduction),
    as.character(data.summary$copy),
    data.summary$microseconds))
```

Let's look at the raw data:

```{r dev='svg'}
ggplot(data=data, aes(x=size, y=microseconds, color=reduction)) +
    theme(legend.position="bottom") +
    facet_grid(copy ~ precision) +
    geom_point(alpha=0.02) + stat_smooth()
```

Ugh, some of the outliers make it hard to see the results, let's fix
that, also remove some weirdness (see later) for small sizes:

```{r dev='png'}
y.max <- ceiling(quantile(data$microseconds, probs=0.999) / 1000) * 1000
data.regular <- subset(data, size >= 2**11)
ggplot(data=data.regular, aes(x=size, y=microseconds, color=lib)) +
    theme(legend.position="bottom") +
    ylim(0, y.max) + facet_grid(copy ~ precision) +
    geom_point(alpha=0.01) + geom_smooth(method="lm")
```

Build a linear model for each reduction, with and without copying:

```{r}
library(plyr)
models <- dlply(data.regular,
    c('reduction', 'copy'), function(df) lm(nanoseconds ~ size, data=df))
models.coef <- arrange(ldply(models, coef), size)
names(models.coef) <- c(
    'reduction', 'copy', 'startup (ns)', 'cost-per-element (ns)')
kable(models.coef)
```

```{r}
models.residuals <- ldply(models, function(x) mean(x$residuals))
names(models.coef) <- c(
    'reduction', 'copy', 'mean(residuals)')
```

Something interesting is happening with small sizes, let's zoom there:

```{r dev='png'}
d <- subset(data, size <= 2**12)
y.max <- ceiling(quantile(d$microseconds, probs=0.999) / 1000) * 1000
cutoffs <- data.frame(cutoff=2**seq(7, 12))
ggplot(data=d, aes(x=size, y=microseconds, color=lib)) +
    theme(legend.position="bottom") +
    ylim(0, y.max) + facet_grid(copy ~ precision) +
    geom_vline(data=cutoffs, aes(xintercept=cutoff), color="blue") +
    geom_point(alpha=0.3)
```
